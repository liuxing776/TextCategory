#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”®è¿è¡Œæ‰€æœ‰å®éªŒçš„è„šæœ¬
æŒ‰ç…§é¢„å®šé¡ºåºæ‰§è¡Œæ‰€æœ‰å®éªŒå¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
"""

import os
import sys
import time
import json
import pandas as pd
from datetime import datetime
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å®éªŒæ¨¡å—
from experiments.baseline_experiment import BaselineExperiment
from experiments.traditional_ml_experiment import TraditionalMLExperiment
from experiments.ablation_experiment import AblationExperiment
from experiments.optimization_experiment import OptimizationExperiment
from experiments.analysis_experiment import AnalysisExperiment
from config import Config


class ExperimentRunner:
    def __init__(self, config):
        self.config = config
        self.results = {}
        self.execution_log = []
        self.start_time = None
        self.end_time = None

    def log_experiment(self, experiment_name, status, duration=None, error=None):
        """è®°å½•å®éªŒæ‰§è¡Œæƒ…å†µ"""
        log_entry = {
            'experiment': experiment_name,
            'status': status,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'duration': duration,
            'error': error
        }
        self.execution_log.append(log_entry)

        if status == 'SUCCESS':
            print(f"âœ… {experiment_name} completed successfully in {duration:.2f}s")
        elif status == 'ERROR':
            print(f"âŒ {experiment_name} failed: {error}")
        else:
            print(f"ğŸ”„ {experiment_name} {status.lower()}")

    def run_baseline_experiment(self):
        """è¿è¡ŒåŸºçº¿å®éªŒ"""
        experiment_name = "Baseline Experiment"
        print(f"\n{'=' * 60}")
        print(f"Starting {experiment_name}")
        print(f"{'=' * 60}")

        start_time = time.time()
        self.log_experiment(experiment_name, 'STARTED')

        try:
            experiment = BaselineExperiment(self.config)
            results = experiment.run_experiment()

            duration = time.time() - start_time
            self.results['baseline'] = results
            self.log_experiment(experiment_name, 'SUCCESS', duration)

            return True

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            self.log_experiment(experiment_name, 'ERROR', duration, error_msg)
            print(f"Error details: {traceback.format_exc()}")
            return False

    def run_traditional_ml_experiment(self):
        """è¿è¡Œä¼ ç»Ÿæœºå™¨å­¦ä¹ å®éªŒ"""
        experiment_name = "Traditional ML Experiment"
        print(f"\n{'=' * 60}")
        print(f"Starting {experiment_name}")
        print(f"{'=' * 60}")

        start_time = time.time()
        self.log_experiment(experiment_name, 'STARTED')

        try:
            experiment = TraditionalMLExperiment(self.config)

            # è¿è¡Œå¿«é€Ÿç­›é€‰ç‰ˆæœ¬ä»¥èŠ‚çœæ—¶é—´
            print("Running quick screening version...")
            results = experiment.run_quick_screening()

            duration = time.time() - start_time
            self.results['traditional_ml'] = results
            self.log_experiment(experiment_name, 'SUCCESS', duration)

            return True

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            self.log_experiment(experiment_name, 'ERROR', duration, error_msg)
            print(f"Error details: {traceback.format_exc()}")
            return False

    def run_ablation_experiment(self):
        """è¿è¡Œæ¶ˆèå®éªŒ"""
        experiment_name = "Ablation Study"
        print(f"\n{'=' * 60}")
        print(f"Starting {experiment_name}")
        print(f"{'=' * 60}")

        start_time = time.time()
        self.log_experiment(experiment_name, 'STARTED')

        try:
            experiment = AblationExperiment(self.config)

            # åªè¿è¡Œéƒ¨åˆ†æ¶ˆèå®éªŒä»¥èŠ‚çœæ—¶é—´
            experiment.load_data()

            # è¿è¡Œé¢„å¤„ç†æ­¥éª¤å®éªŒ
            preprocessing_results = experiment.experiment_preprocessing_steps()

            # è¿è¡Œç‰¹å¾é€‰æ‹©å®éªŒ
            feature_selection_results = experiment.experiment_feature_selection()

            results = {
                'preprocessing': preprocessing_results,
                'feature_selection': feature_selection_results
            }

            duration = time.time() - start_time
            self.results['ablation'] = results
            self.log_experiment(experiment_name, 'SUCCESS', duration)

            return True

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            self.log_experiment(experiment_name, 'ERROR', duration, error_msg)
            print(f"Error details: {traceback.format_exc()}")
            return False

    def run_optimization_experiment(self):
        """è¿è¡Œä¼˜åŒ–å®éªŒ"""
        experiment_name = "Optimization Experiment"
        print(f"\n{'=' * 60}")
        print(f"Starting {experiment_name}")
        print(f"{'=' * 60}")

        start_time = time.time()
        self.log_experiment(experiment_name, 'STARTED')

        try:
            experiment = OptimizationExperiment(self.config)
            experiment.load_data()

            # è¿è¡Œç½‘æ ¼æœç´¢ä¼˜åŒ–
            results = experiment.grid_search_optimization()

            duration = time.time() - start_time
            self.results['optimization'] = results
            self.log_experiment(experiment_name, 'SUCCESS', duration)

            return True

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            self.log_experiment(experiment_name, 'ERROR', duration, error_msg)
            print(f"Error details: {traceback.format_exc()}")
            return False

    def run_analysis_experiment(self):
        """è¿è¡Œåˆ†æå®éªŒ"""
        experiment_name = "Analysis Experiment"
        print(f"\n{'=' * 60}")
        print(f"Starting {experiment_name}")
        print(f"{'=' * 60}")

        start_time = time.time()
        self.log_experiment(experiment_name, 'STARTED')

        try:
            experiment = AnalysisExperiment(self.config)
            results = experiment.generate_comprehensive_analysis_report()

            duration = time.time() - start_time
            self.results['analysis'] = results
            self.log_experiment(experiment_name, 'SUCCESS', duration)

            return True

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            self.log_experiment(experiment_name, 'ERROR', duration, error_msg)
            print(f"Error details: {traceback.format_exc()}")
            return False

    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""
        print(f"\n{'=' * 60}")
        print("Generating Final Comprehensive Report")
        print(f"{'=' * 60}")

        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = f"results/final_report_{timestamp}"
        os.makedirs(report_dir, exist_ok=True)

        # 1. ç”Ÿæˆæ‰§è¡Œæ—¥å¿—æŠ¥å‘Š
        self.generate_execution_log_report(report_dir)

        # 2. ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
        self.generate_performance_comparison_report(report_dir)

        # 3. ç”Ÿæˆæœ€ä½³æ¨¡å‹æŠ¥å‘Š
        self.generate_best_model_report(report_dir)

        # 4. ç”Ÿæˆå»ºè®®å’Œç»“è®º
        self.generate_recommendations_report(report_dir)

        print(f"\nFinal report generated in: {report_dir}")

        return report_dir

    def generate_execution_log_report(self, report_dir):
        """ç”Ÿæˆæ‰§è¡Œæ—¥å¿—æŠ¥å‘Š"""
        # ä¿å­˜æ‰§è¡Œæ—¥å¿—
        log_df = pd.DataFrame(self.execution_log)
        log_df.to_csv(f"{report_dir}/execution_log.csv", index=False)

        # ç”Ÿæˆæ‘˜è¦
        successful_experiments = len([log for log in self.execution_log if log['status'] == 'SUCCESS'])
        total_experiments = len([log for log in self.execution_log if log['status'] in ['SUCCESS', 'ERROR']])
        total_duration = self.end_time - self.start_time if self.end_time and self.start_time else 0

        summary = {
            'total_experiments': total_experiments,
            'successful_experiments': successful_experiments,
            'failed_experiments': total_experiments - successful_experiments,
            'success_rate': successful_experiments / total_experiments if total_experiments > 0 else 0,
            'total_duration_minutes': total_duration / 60,
            'start_time': self.start_time.strftime("%Y-%m-%d %H:%M:%S") if self.start_time else None,
            'end_time': self.end_time.strftime("%Y-%m-%d %H:%M:%S") if self.end_time else None
        }

        with open(f"{report_dir}/execution_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"âœ… Execution log saved: {successful_experiments}/{total_experiments} experiments successful")

    def generate_performance_comparison_report(self, report_dir):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
        performance_data = []

        # ä»å„ä¸ªå®éªŒç»“æœä¸­æå–æ€§èƒ½æ•°æ®
        if 'baseline' in self.results:
            baseline_results = self.results['baseline']
            if isinstance(baseline_results, pd.DataFrame):
                best_baseline = baseline_results.sort_values('Test Accuracy', ascending=False).iloc[0]
                performance_data.append({
                    'Experiment': 'Baseline',
                    'Method': f"{best_baseline.get('Feature Method', 'Unknown')} + {best_baseline.get('Model', 'Unknown')}",
                    'Test Accuracy': best_baseline.get('Test Accuracy', 0),
                    'Test F1': best_baseline.get('Test F1', 0)
                })

        if 'traditional_ml' in self.results:
            traditional_results = self.results['traditional_ml']
            if isinstance(traditional_results, pd.DataFrame) and not traditional_results.empty:
                best_traditional = traditional_results.sort_values('Test Accuracy', ascending=False).iloc[0]
                performance_data.append({
                    'Experiment': 'Traditional ML',
                    'Method': f"{best_traditional.get('Feature Extractor', 'Unknown')} + {best_traditional.get('Classifier', 'Unknown')}",
                    'Test Accuracy': best_traditional.get('Test Accuracy', 0),
                    'Test F1': best_traditional.get('Test F1', 0)
                })

        if 'optimization' in self.results:
            opt_results = self.results['optimization']
            if 'test_metrics' in opt_results:
                performance_data.append({
                    'Experiment': 'Optimized',
                    'Method': 'Grid Search Optimized',
                    'Test Accuracy': opt_results['test_metrics'].get('accuracy', 0),
                    'Test F1': opt_results['test_metrics'].get('f1_macro', 0)
                })

        # ä¿å­˜æ€§èƒ½å¯¹æ¯”
        if performance_data:
            performance_df = pd.DataFrame(performance_data)
            performance_df = performance_df.sort_values('Test Accuracy', ascending=False)
            performance_df.to_csv(f"{report_dir}/performance_comparison.csv", index=False)

            # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
            best_method = performance_df.iloc[0]

            print(f"âœ… Best performing method:")
            print(f"   Experiment: {best_method['Experiment']}")
            print(f"   Method: {best_method['Method']}")
            print(f"   Test Accuracy: {best_method['Test Accuracy']:.4f}")
            print(f"   Test F1: {best_method['Test F1']:.4f}")
        else:
            print("âš ï¸  No performance data available for comparison")

    def generate_best_model_report(self, report_dir):
        """ç”Ÿæˆæœ€ä½³æ¨¡å‹æŠ¥å‘Š"""
        best_model_info = {
            'model_selection_criteria': 'Highest test accuracy',
            'recommendations': [],
            'key_findings': []
        }

        # ä»æ¶ˆèå®éªŒä¸­æå–å…³é”®å‘ç°
        if 'ablation' in self.results:
            ablation_results = self.results['ablation']

            if 'preprocessing' in ablation_results:
                prep_results = ablation_results['preprocessing']
                best_prep = prep_results.sort_values('Test Accuracy', ascending=False).iloc[0]
                best_model_info['recommendations'].append(
                    f"Best preprocessing: {best_prep['Preprocessing']} (Accuracy: {best_prep['Test Accuracy']:.4f})"
                )

            if 'feature_selection' in ablation_results:
                feat_results = ablation_results['feature_selection']
                best_feat = feat_results.sort_values('Test Accuracy', ascending=False).iloc[0]
                best_model_info['recommendations'].append(
                    f"Best feature selection: {best_feat['Feature Selection']} (Accuracy: {best_feat['Test Accuracy']:.4f})"
                )

        # ä»åˆ†æå®éªŒä¸­æå–å…³é”®å‘ç°
        if 'analysis' in self.results:
            analysis_results = self.results['analysis']

            if 'confidence_analysis' in analysis_results:
                conf_analysis = analysis_results['confidence_analysis']
                best_model_info['key_findings'].append(
                    f"Model achieves {conf_analysis['overall_accuracy']:.4f} overall accuracy"
                )
                best_model_info['key_findings'].append(
                    f"High confidence predictions (>0.9): {conf_analysis['high_confidence_count']} samples with {conf_analysis['high_confidence_accuracy']:.4f} accuracy"
                )

        # ä¿å­˜æœ€ä½³æ¨¡å‹ä¿¡æ¯
        with open(f"{report_dir}/best_model_report.json", 'w') as f:
            json.dump(best_model_info, f, indent=2, ensure_ascii=False)

        print("âœ… Best model report generated")

    def generate_recommendations_report(self, report_dir):
        """ç”Ÿæˆå»ºè®®å’Œç»“è®ºæŠ¥å‘Š"""
        recommendations = {
            'project_summary': {
                'dataset': 'ä»Šæ—¥å¤´æ¡ä¸­æ–‡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†',
                'classes': 15,
                'total_samples': 382688,
                'task': 'çŸ­æ–‡æœ¬å¤šåˆ†ç±»'
            },
            'methodology_recommendations': [
                'åŸºäºå®éªŒç»“æœï¼Œæ¨èä½¿ç”¨TF-IDFç‰¹å¾æå–é…åˆLogistic Regression',
                'é¢„å¤„ç†æ­¥éª¤å»ºè®®åŒ…æ‹¬ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ã€æ•°å­—æ¸…ç†ã€jiebaåˆ†è¯',
                'ç‰¹å¾é€‰æ‹©å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨Chi2æ–¹æ³•é€‰æ‹©10000-15000ä¸ªç‰¹å¾',
                'ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¯é€šè¿‡è°ƒæ•´ç±»åˆ«æƒé‡æˆ–é‡é‡‡æ ·æ–¹æ³•è§£å†³'
            ],
            'performance_insights': [
                'åŸºçº¿æ¨¡å‹å·²èƒ½è¾¾åˆ°è¾ƒå¥½çš„æ€§èƒ½ï¼Œè¯´æ˜ä»»åŠ¡å…·æœ‰ä¸€å®šçš„å¯è§£æ€§',
                'ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ·±åº¦å­¦ä¹ å¯èƒ½ä¸æ˜¯å¿…éœ€çš„',
                'é¢„å¤„ç†æ­¥éª¤å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“',
                'ç‰¹å¾å·¥ç¨‹æ¯”æ¨¡å‹å¤æ‚åº¦æ›´é‡è¦'
            ],
            'future_improvements': [
                'å¯ä»¥å°è¯•æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼ˆå¦‚è¯å‘é‡ã€BERTç­‰ï¼‰',
                'é›†æˆå­¦ä¹ æ–¹æ³•å¯èƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½',
                'é”™è¯¯åˆ†ææ˜¾ç¤ºæŸäº›ç±»åˆ«å®¹æ˜“æ··æ·†ï¼Œå¯é’ˆå¯¹æ€§æ”¹è¿›',
                'å¢åŠ æ›´å¤šæ•°æ®å¯èƒ½æ”¹å–„å°‘æ•°ç±»åˆ«çš„æ€§èƒ½'
            ],
            'technical_considerations': [
                'æ¨¡å‹è®­ç»ƒæ—¶é—´å’Œé¢„æµ‹é€Ÿåº¦éœ€è¦å¹³è¡¡',
                'å†…å­˜ä½¿ç”¨é‡éœ€è¦è€ƒè™‘ï¼Œç‰¹åˆ«æ˜¯ç‰¹å¾æ•°é‡è¾ƒå¤šæ—¶',
                'æ¨¡å‹çš„å¯è§£é‡Šæ€§åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½å¾ˆé‡è¦',
                'å®šæœŸé‡æ–°è®­ç»ƒä»¥é€‚åº”æ•°æ®åˆ†å¸ƒå˜åŒ–'
            ]
        }

        # ä¿å­˜å»ºè®®æŠ¥å‘Š
        with open(f"{report_dir}/recommendations.json", 'w', encoding='utf-8') as f:
            json.dump(recommendations, f, indent=2, ensure_ascii=False)

        # ç”Ÿæˆç®€åŒ–ç‰ˆçš„README
        readme_content = f"""# ä¸­æ–‡æ–‡æœ¬åˆ†ç±»é¡¹ç›®å®éªŒæŠ¥å‘Š

## é¡¹ç›®æ¦‚è¿°
- æ•°æ®é›†ï¼šä»Šæ—¥å¤´æ¡ä¸­æ–‡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†
- ç±»åˆ«æ•°ï¼š15ä¸ª
- æ€»æ ·æœ¬æ•°ï¼š382,688
- ä»»åŠ¡ç±»å‹ï¼šçŸ­æ–‡æœ¬å¤šåˆ†ç±»

## å®éªŒç»“æœæ‘˜è¦
æœ¬é¡¹ç›®å®Œæˆäº†ä»¥ä¸‹å®éªŒï¼š
1. åŸºçº¿å®éªŒ - å»ºç«‹æ€§èƒ½åŸºå‡†
2. ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¯¹æ¯” - å¤šç§ç®—æ³•æ¯”è¾ƒ
3. æ¶ˆèå®éªŒ - åˆ†æå„ç»„ä»¶å½±å“
4. ä¼˜åŒ–å®éªŒ - è¶…å‚æ•°è°ƒä¼˜
5. åˆ†æå®éªŒ - æ·±å…¥æ€§èƒ½åˆ†æ

## ä¸»è¦å‘ç°
- TF-IDF + Logistic Regression ç»„åˆè¡¨ç°æœ€ä½³
- é¢„å¤„ç†æ­¥éª¤å¯¹æ€§èƒ½å½±å“æ˜¾è‘—
- ç‰¹å¾é€‰æ‹©å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½
- ç±»åˆ«ä¸å¹³è¡¡æ˜¯éœ€è¦å…³æ³¨çš„é—®é¢˜

## æ¨èé…ç½®
- ç‰¹å¾æå–ï¼šTF-IDF (1-2gram, max_features=15000)
- åˆ†ç±»å™¨ï¼šLogistic Regression (C=1, class_weight='balanced')
- é¢„å¤„ç†ï¼šå»æ ‡ç‚¹ + å»æ•°å­— + jiebaåˆ†è¯
- ç‰¹å¾é€‰æ‹©ï¼šChi2é€‰æ‹©10000ä¸ªç‰¹å¾

## æ–‡ä»¶è¯´æ˜
- execution_log.csv: å®éªŒæ‰§è¡Œè®°å½•
- performance_comparison.csv: æ€§èƒ½å¯¹æ¯”ç»“æœ
- best_model_report.json: æœ€ä½³æ¨¡å‹è¯¦æƒ…
- recommendations.json: è¯¦ç»†å»ºè®®å’Œæ”¹è¿›æ–¹å‘

æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""

        with open(f"{report_dir}/README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print("âœ… Recommendations and conclusions generated")

    def run_all_experiments(self, skip_failed=True):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        self.start_time = datetime.now()

        print("ğŸš€ Starting comprehensive text classification experiments...")
        print(f"Start time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # å®éªŒåˆ—è¡¨
        experiments = [
            ('baseline', self.run_baseline_experiment),
            ('traditional_ml', self.run_traditional_ml_experiment),
            ('ablation', self.run_ablation_experiment),
            ('optimization', self.run_optimization_experiment),
            ('analysis', self.run_analysis_experiment)
        ]

        # é€ä¸ªè¿è¡Œå®éªŒ
        for exp_name, exp_func in experiments:
            success = exp_func()

            if not success and not skip_failed:
                print(f"âŒ Stopping due to failure in {exp_name}")
                break

        self.end_time = datetime.now()
        total_duration = (self.end_time - self.start_time).total_seconds()

        print(f"\nğŸ‰ All experiments completed!")
        print(f"End time: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total duration: {total_duration / 60:.2f} minutes")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_dir = self.generate_final_report()

        return report_dir


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ä¸­æ–‡æ–‡æœ¬åˆ†ç±»é¡¹ç›® - å…¨é¢å®éªŒå¥—ä»¶")
    print("=" * 80)

    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='Run text classification experiments')
    parser.add_argument('--experiments', nargs='*',
                        choices=['baseline', 'traditional_ml', 'ablation', 'optimization', 'analysis', 'all'],
                        default=['all'],
                        help='Experiments to run')
    parser.add_argument('--skip-failed', action='store_true', default=True,
                        help='Continue with other experiments if one fails')

    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    config = Config()
    runner = ExperimentRunner(config)

    # è¿è¡Œé€‰å®šçš„å®éªŒ
    if 'all' in args.experiments:
        runner.run_all_experiments(skip_failed=args.skip_failed)
    else:
        runner.start_time = datetime.now()

        for exp_name in args.experiments:
            if exp_name == 'baseline':
                runner.run_baseline_experiment()
            elif exp_name == 'traditional_ml':
                runner.run_traditional_ml_experiment()
            elif exp_name == 'ablation':
                runner.run_ablation_experiment()
            elif exp_name == 'optimization':
                runner.run_optimization_experiment()
            elif exp_name == 'analysis':
                runner.run_analysis_experiment()

        runner.end_time = datetime.now()
        runner.generate_final_report()


if __name__ == "__main__":
    main()