#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»Šæ—¥å¤´æ¡æ–‡æœ¬åˆ†ç±»æ•°æ®é›†åˆ†æ - ç²¾ç®€ç‰ˆ
ä¸“æ³¨äºTextCNNå®éªŒæ‰€éœ€çš„æ ¸å¿ƒä¿¡æ¯
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import re
import os
from collections import Counter
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


class SimpleDataExplorer:
    """ç²¾ç®€ç‰ˆæ•°æ®æ¢ç´¢å™¨ - ä¸“æ³¨äºTextCNNå®éªŒæ ¸å¿ƒéœ€æ±‚"""

    def __init__(self, data_path, output_dir="results"):
        self.data_path = data_path
        self.output_dir = output_dir
        self.df = None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        self.category_mapping = {
            '100': 'æ°‘ç”Ÿæ•…äº‹', '101': 'æ–‡åŒ–', '102': 'å¨±ä¹', '103': 'ä½“è‚²',
            '104': 'è´¢ç»', '106': 'æˆ¿äº§', '107': 'æ±½è½¦', '108': 'æ•™è‚²',
            '109': 'ç§‘æŠ€', '110': 'å†›äº‹', '112': 'æ—…æ¸¸', '113': 'å›½é™…',
            '114': 'è¯åˆ¸', '115': 'å†œä¸š', '116': 'æ¸¸æˆ'
        }

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        data_list = []
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    parts = line.strip().split('_!_')
                    if len(parts) >= 4:
                        data_list.append({
                            'category_code': parts[1],
                            'category_name': parts[2],
                            'title': parts[3]
                        })

        self.df = pd.DataFrame(data_list)
        self.df = self.df.dropna(subset=['title'])
        self.df = self.df[self.df['title'].str.len() > 5]

        # æ·»åŠ ä¸­æ–‡ç±»åˆ«å
        self.df['category_chinese'] = self.df['category_code'].map(self.category_mapping)

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.df):,} æ¡è®°å½•")
        return self.df

    def analyze_for_textcnn(self):
        """ä¸“é—¨ä¸ºTextCNNå®éªŒåˆ†æå…³é”®ä¿¡æ¯"""
        print("\n" + "=" * 50)
        print("ğŸ¯ TextCNNå®éªŒå…³é”®ä¿¡æ¯åˆ†æ")
        print("=" * 50)

        results = {}

        # 1. åŸºç¡€ç»Ÿè®¡
        total_samples = len(self.df)
        num_classes = self.df['category_code'].nunique()

        print(f"ğŸ“Š åŸºç¡€ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"  ç±»åˆ«æ•°: {num_classes}")

        results['total_samples'] = total_samples
        results['num_classes'] = num_classes

        # 2. ç±»åˆ«åˆ†å¸ƒ - æ ¸å¿ƒï¼šæ•°æ®ä¸å¹³è¡¡é—®é¢˜
        category_counts = self.df['category_chinese'].value_counts()
        max_samples = category_counts.max()
        min_samples = category_counts.min()
        imbalance_ratio = max_samples / min_samples

        print(f"\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒï¼ˆå½±å“é‡‡æ ·å’ŒæŸå¤±å‡½æ•°è®¾è®¡ï¼‰:")
        print(f"  æ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.1f}:1")
        print(f"  æœ€å¤šç±»åˆ«: {category_counts.index[0]} ({max_samples:,} æ ·æœ¬)")
        print(f"  æœ€å°‘ç±»åˆ«: {category_counts.index[-1]} ({min_samples:,} æ ·æœ¬)")

        # ä¿å­˜ç±»åˆ«åˆ†å¸ƒ - ç”¨äºåç»­æ•°æ®åˆ’åˆ†
        category_dist = pd.DataFrame({
            'ç±»åˆ«åç§°': category_counts.index,
            'æ ·æœ¬æ•°é‡': category_counts.values,
            'å æ¯”ç™¾åˆ†æ¯”': category_counts.values / total_samples * 100
        })
        category_dist.to_csv(f"{self.output_dir}/category_distribution.csv", index=False, encoding='utf-8-sig')
        print(f"  âœ… ä¿å­˜ç±»åˆ«åˆ†å¸ƒ: {self.output_dir}/category_distribution.csv")

        results['imbalance_ratio'] = imbalance_ratio
        results['max_category'] = category_counts.index[0]
        results['min_category'] = category_counts.index[-1]

        # 3. æ–‡æœ¬é•¿åº¦åˆ†æ - æ ¸å¿ƒï¼šç¡®å®šmax_lengthå‚æ•°
        self.df['title_length'] = self.df['title'].str.len()
        length_stats = self.df['title_length'].describe()

        # å…³é”®ç™¾åˆ†ä½æ•°
        p90 = int(self.df['title_length'].quantile(0.90))
        p95 = int(self.df['title_length'].quantile(0.95))
        p99 = int(self.df['title_length'].quantile(0.99))

        print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦åˆ†æï¼ˆç¡®å®šmax_lengthå‚æ•°ï¼‰:")
        print(f"  å¹³å‡é•¿åº¦: {length_stats['mean']:.1f}")
        print(f"  90%æ–‡æœ¬é•¿åº¦ â‰¤ {p90} å­—ç¬¦  [æ¨èç”¨äºå¿«é€Ÿå®éªŒ]")
        print(f"  95%æ–‡æœ¬é•¿åº¦ â‰¤ {p95} å­—ç¬¦  [æ¨èç”¨äºæ­£å¼å®éªŒ]")
        print(f"  99%æ–‡æœ¬é•¿åº¦ â‰¤ {p99} å­—ç¬¦  [å®Œæ•´è¦†ç›–ä½†å¯èƒ½è¿‡é•¿]")

        # ä¿å­˜é•¿åº¦ç»Ÿè®¡
        length_summary = pd.DataFrame({
            'ç»Ÿè®¡æŒ‡æ ‡': ['å¹³å‡å€¼', 'ä¸­ä½æ•°', '90%åˆ†ä½æ•°', '95%åˆ†ä½æ•°', '99%åˆ†ä½æ•°', 'æœ€å¤§å€¼'],
            'æ•°å€¼': [length_stats['mean'], length_stats['50%'], p90, p95, p99, length_stats['max']],
            'è¯´æ˜': ['æ‰€æœ‰æ ‡é¢˜çš„å¹³å‡é•¿åº¦', 'ä¸­ä½æ•°é•¿åº¦', '90%æ ‡é¢˜åœ¨æ­¤é•¿åº¦ä»¥ä¸‹', 'æ¨èæœ€å¤§é•¿åº¦', 'å‡ ä¹å…¨è¦†ç›–é•¿åº¦', 'æœ€é•¿æ ‡é¢˜é•¿åº¦']
        })
        length_summary.to_csv(f"{self.output_dir}/length_analysis.csv", index=False, encoding='utf-8-sig')
        print(f"  âœ… ä¿å­˜é•¿åº¦åˆ†æ: {self.output_dir}/length_analysis.csv")

        results['recommended_max_length'] = p95
        results['avg_length'] = length_stats['mean']

        # 4. è¯æ±‡åˆ†æ - æ ¸å¿ƒï¼šç¡®å®švocab_sizeå‚æ•°
        print(f"\nğŸ”¤ è¯æ±‡åˆ†æï¼ˆç¡®å®švocab_sizeå‚æ•°ï¼‰:")

        # åˆ†è¯å¹¶ç»Ÿè®¡
        all_words = []
        for title in self.df['title'].sample(min(50000, len(self.df))):  # é‡‡æ ·åŠ é€Ÿ
            words = jieba.lcut(title)
            words = [w for w in words if len(w) >= 2 and not re.match(r'^[\d\W]+$', w)]
            all_words.extend(words)

        word_counter = Counter(all_words)
        total_words = len(all_words)
        unique_words = len(word_counter)

        print(f"  æ€»è¯æ•°: {total_words:,}")
        print(f"  å”¯ä¸€è¯æ•°: {unique_words:,}")

        # è®¡ç®—ä¸åŒè¯æ±‡è¡¨å¤§å°çš„è¦†ç›–ç‡
        vocab_sizes = [5000, 10000, 20000, 30000, 50000]
        coverage_info = []

        cumsum = 0
        for i, (word, count) in enumerate(word_counter.most_common(), 1):
            cumsum += count
            if i in vocab_sizes:
                coverage = cumsum / total_words * 100
                coverage_info.append({'è¯æ±‡è¡¨å¤§å°': i, 'è¦†ç›–ç‡ç™¾åˆ†æ¯”': coverage})
                print(f"  è¯æ±‡è¡¨ {i:,}: è¦†ç›– {coverage:.1f}% è¯æ±‡")

        # ä¿å­˜è¯æ±‡è¦†ç›–ç‡åˆ†æ
        coverage_df = pd.DataFrame(coverage_info)
        coverage_df.to_csv(f"{self.output_dir}/vocab_coverage.csv", index=False, encoding='utf-8-sig')
        print(f"  âœ… ä¿å­˜è¯æ±‡åˆ†æ: {self.output_dir}/vocab_coverage.csv")

        # æ¨èè¯æ±‡è¡¨å¤§å°
        recommended_vocab = 30000 if unique_words > 30000 else min(unique_words, 20000)
        results['recommended_vocab_size'] = recommended_vocab
        results['unique_words'] = unique_words

        print(f"  ğŸ’¡ æ¨èè¯æ±‡è¡¨å¤§å°: {recommended_vocab:,}")

        # 5. ç”ŸæˆTextCNNæ¨èå‚æ•°
        print(f"\nğŸš€ TextCNNæ¨èå‚æ•°:")
        textcnn_params = {
            'vocab_size': recommended_vocab,
            'embed_dim': 300,
            'max_length': p95,
            'num_classes': num_classes,
            'filter_sizes': [3, 4, 5],
            'num_filters': 128,
            'dropout': 0.5,
            'batch_size': 64 if total_samples > 100000 else 32,
            'learning_rate': 0.001
        }

        for param, value in textcnn_params.items():
            print(f"  {param}: {value}")

        # ä¿å­˜æ¨èå‚æ•°
        params_df = pd.DataFrame([
            {'å‚æ•°åç§°': k, 'å‚æ•°å€¼': v, 'å‚æ•°è¯´æ˜': self._get_param_description(k)}
            for k, v in textcnn_params.items()
        ])
        params_df.to_csv(f"{self.output_dir}/textcnn_params.csv", index=False, encoding='utf-8-sig')
        print(f"  âœ… ä¿å­˜æ¨èå‚æ•°: {self.output_dir}/textcnn_params.csv")

        # 6. æ•°æ®å¤„ç†å»ºè®®
        print(f"\nğŸ’¡ å…³é”®å»ºè®®:")
        suggestions = []

        if imbalance_ratio > 10:
            suggestions.append("ä¸¥é‡æ•°æ®ä¸å¹³è¡¡ï¼Œå»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–focal loss")
        elif imbalance_ratio > 5:
            suggestions.append("ä¸­ç­‰æ•°æ®ä¸å¹³è¡¡ï¼Œå»ºè®®è°ƒæ•´ç±»åˆ«æƒé‡")
        else:
            suggestions.append("æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡è¡¡")

        if p95 > 100:
            suggestions.append(f"æ–‡æœ¬è¾ƒé•¿ï¼Œå»ºè®®max_length={p95}å¹¶è€ƒè™‘æˆªæ–­ç­–ç•¥")
        else:
            suggestions.append(f"æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼Œå»ºè®®max_length={p95}")

        suggestions.append("ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒä¸€è‡´")
        suggestions.append("é‡ç‚¹å…³æ³¨å°‘æ•°ç±»åˆ«çš„F1-score")

        for i, suggestion in enumerate(suggestions, 1):
            print(f"  {i}. {suggestion}")

        # ä¿å­˜åˆ†æç»“æœæ‘˜è¦
        results.update(textcnn_params)
        results['suggestions'] = suggestions

        summary_df = pd.DataFrame([
            {'ç»Ÿè®¡æŒ‡æ ‡': 'æ€»æ ·æœ¬æ•°', 'æ•°å€¼': total_samples, 'è¯´æ˜': 'æ•°æ®é›†ä¸­çš„æ€»æ–°é—»æ ‡é¢˜æ•°é‡'},
            {'ç»Ÿè®¡æŒ‡æ ‡': 'ç±»åˆ«æ•°é‡', 'æ•°å€¼': num_classes, 'è¯´æ˜': 'æ–°é—»åˆ†ç±»çš„ç±»åˆ«æ€»æ•°'},
            {'ç»Ÿè®¡æŒ‡æ ‡': 'æ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹', 'æ•°å€¼': f"{imbalance_ratio:.1f}:1", 'è¯´æ˜': 'æœ€å¤šç±»åˆ«ä¸æœ€å°‘ç±»åˆ«çš„æ ·æœ¬æ•°æ¯”ä¾‹'},
            {'ç»Ÿè®¡æŒ‡æ ‡': 'æ¨èæœ€å¤§é•¿åº¦', 'æ•°å€¼': p95, 'è¯´æ˜': 'TextCNNæ¨¡å‹å»ºè®®çš„max_lengthå‚æ•°'},
            {'ç»Ÿè®¡æŒ‡æ ‡': 'æ¨èè¯æ±‡è¡¨å¤§å°', 'æ•°å€¼': recommended_vocab, 'è¯´æ˜': 'TextCNNæ¨¡å‹å»ºè®®çš„vocab_sizeå‚æ•°'},
            {'ç»Ÿè®¡æŒ‡æ ‡': 'å¹³å‡æ ‡é¢˜é•¿åº¦', 'æ•°å€¼': f"{length_stats['mean']:.1f}", 'è¯´æ˜': 'æ‰€æœ‰æ–°é—»æ ‡é¢˜çš„å¹³å‡å­—ç¬¦é•¿åº¦'}
        ])
        summary_df.to_csv(f"{self.output_dir}/analysis_summary.csv", index=False, encoding='utf-8-sig')
        print(f"  âœ… ä¿å­˜åˆ†ææ‘˜è¦: {self.output_dir}/analysis_summary.csv")

        return results

    def _get_param_description(self, param):
        """å‚æ•°è¯´æ˜ - ä¸­æ–‡ç‰ˆ"""
        descriptions = {
            'vocab_size': 'è¯æ±‡è¡¨å¤§å°ï¼ŒåŸºäºè¯æ±‡è¦†ç›–ç‡åˆ†æç¡®å®š',
            'embed_dim': 'è¯å‘é‡ç»´åº¦ï¼Œ300æ˜¯é¢„è®­ç»ƒè¯å‘é‡çš„å¸¸ç”¨ç»´åº¦',
            'max_length': 'æœ€å¤§åºåˆ—é•¿åº¦ï¼ŒåŸºäº95%åˆ†ä½æ•°ç¡®å®šï¼Œå¹³è¡¡è¦†ç›–ç‡å’Œæ•ˆç‡',
            'num_classes': 'åˆ†ç±»ç±»åˆ«æ•°é‡ï¼Œç­‰äºæ•°æ®é›†ä¸­çš„æ–°é—»ç±»åˆ«æ€»æ•°',
            'filter_sizes': 'å·ç§¯æ ¸å°ºå¯¸åˆ—è¡¨ï¼Œç”¨äºæ•è·ä¸åŒé•¿åº¦çš„æ–‡æœ¬ç‰¹å¾',
            'num_filters': 'æ¯ç§å°ºå¯¸å·ç§¯æ ¸çš„æ•°é‡ï¼Œå½±å“æ¨¡å‹ç‰¹å¾æå–èƒ½åŠ›',
            'dropout': 'éšæœºå¤±æ´»æ¯”ä¾‹ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ',
            'batch_size': 'æ‰¹æ¬¡å¤§å°ï¼ŒåŸºäºæ•°æ®é›†è§„æ¨¡å’Œå†…å­˜é™åˆ¶è°ƒæ•´',
            'learning_rate': 'å­¦ä¹ ç‡ï¼ŒAdamä¼˜åŒ–å™¨çš„å¸¸ç”¨åˆå§‹å€¼'
        }
        return descriptions.get(param, 'æ¨¡å‹å‚æ•°é…ç½®')

    def create_essential_plots(self):
        """ç”Ÿæˆ3ä¸ªæ ¸å¿ƒå›¾è¡¨"""
        print(f"\nğŸ“Š ç”Ÿæˆæ ¸å¿ƒå¯è§†åŒ–å›¾è¡¨...")

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. ç±»åˆ«åˆ†å¸ƒ
        category_counts = self.df['category_chinese'].value_counts()
        axes[0, 0].bar(range(len(category_counts)), category_counts.values)
        axes[0, 0].set_title('ç±»åˆ«åˆ†å¸ƒ - è¯„ä¼°æ•°æ®ä¸å¹³è¡¡', fontweight='bold')
        axes[0, 0].set_xticks(range(len(category_counts)))
        axes[0, 0].set_xticklabels(category_counts.index, rotation=45)
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')

        # 2. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
        axes[0, 1].hist(self.df['title_length'], bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].axvline(self.df['title_length'].quantile(0.95), color='red', linestyle='--',
                           label=f'95%åˆ†ä½æ•°: {int(self.df["title_length"].quantile(0.95))}')
        axes[0, 1].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ - ç¡®å®šmax_length', fontweight='bold')
        axes[0, 1].set_xlabel('æ ‡é¢˜é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].legend()

        # 3. ç±»åˆ«æ ·æœ¬æ•°å¯¹æ¯”ï¼ˆæ°´å¹³æ¡å½¢å›¾ï¼‰
        sorted_counts = category_counts.sort_values()
        axes[1, 0].barh(range(len(sorted_counts)), sorted_counts.values)
        axes[1, 0].set_title('ç±»åˆ«ä¸å¹³è¡¡ç¨‹åº¦', fontweight='bold')
        axes[1, 0].set_yticks(range(len(sorted_counts)))
        axes[1, 0].set_yticklabels(sorted_counts.index)
        axes[1, 0].set_xlabel('æ ·æœ¬æ•°é‡')

        # 4. é•¿åº¦åˆ†å¸ƒç®±çº¿å›¾
        length_data = [self.df[self.df['category_chinese'] == cat]['title_length'].values
                       for cat in category_counts.index[:8]]  # åªæ˜¾ç¤ºå‰8ä¸ªç±»åˆ«
        axes[1, 1].boxplot(length_data, labels=category_counts.index[:8])
        axes[1, 1].set_title('å„ç±»åˆ«é•¿åº¦åˆ†å¸ƒ', fontweight='bold')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].set_ylabel('æ ‡é¢˜é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰')

        plt.tight_layout()
        plot_path = f"{self.output_dir}/essential_analysis.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… ä¿å­˜æ ¸å¿ƒå›¾è¡¨: {plot_path}")


def main():
    """ä¸»å‡½æ•°"""
    data_path = "D:\\g3\\ML\\TextCategory2\\toutiao_cat_data.txt"  # è¯·ä¿®æ”¹ä¸ºå®é™…è·¯å¾„

    try:
        print("ğŸš€ å¼€å§‹ä»Šæ—¥å¤´æ¡æ•°æ®é›†åˆ†æ - TextCNNä¸“ç”¨ç‰ˆ")
        print("=" * 50)

        # åˆ›å»ºåˆ†æå™¨
        explorer = SimpleDataExplorer(data_path)

        # åŠ è½½æ•°æ®
        df = explorer.load_data()

        # æ ¸å¿ƒåˆ†æ
        results = explorer.analyze_for_textcnn()

        # ç”Ÿæˆå›¾è¡¨
        explorer.create_essential_plots()

        print(f"\nâœ… åˆ†æå®Œæˆï¼ç”Ÿæˆæ–‡ä»¶:")
        print(f"ğŸ“Š category_distribution.csv - ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡ï¼ˆç”¨äºæ•°æ®åˆ’åˆ†ï¼‰")
        print(f"ğŸ“ length_analysis.csv - æ–‡æœ¬é•¿åº¦åˆ†æï¼ˆç¡®å®šmax_lengthå‚æ•°ï¼‰")
        print(f"ğŸ”¤ vocab_coverage.csv - è¯æ±‡è¦†ç›–ç‡åˆ†æï¼ˆç¡®å®švocab_sizeå‚æ•°ï¼‰")
        print(f"âš™ï¸  textcnn_params.csv - TextCNNæ¨èå‚æ•°é…ç½®")
        print(f"ğŸ“‹ analysis_summary.csv - æ•°æ®åˆ†ææ‘˜è¦æŠ¥å‘Š")
        print(f"ğŸ“ˆ essential_analysis.png - æ ¸å¿ƒå¯è§†åŒ–å›¾è¡¨")

        print(f"\nğŸ¯ æ¥ä¸‹æ¥å¯ä»¥:")
        print(f"1. ä½¿ç”¨æ¨èå‚æ•°æ„å»ºTextCNNæ¨¡å‹")
        print(f"2. åŸºäºç±»åˆ«åˆ†å¸ƒè¿›è¡Œæ•°æ®åˆ’åˆ†")
        print(f"3. æ ¹æ®ä¸å¹³è¡¡ç¨‹åº¦è®¾è®¡æŸå¤±å‡½æ•°")
        print(f"4. å¼€å§‹æ¨¡å‹è®­ç»ƒå’Œå®éªŒ")

        return explorer

    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return None
    except Exception as e:
        print(f"âŒ åˆ†æå‡ºé”™: {str(e)}")
        return None


if __name__ == "__main__":
    explorer = main()